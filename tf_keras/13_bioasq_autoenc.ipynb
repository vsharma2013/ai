{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "### hack tf-keras to appear as top level keras\n",
    "import sys\n",
    "sys.modules['keras'] = keras\n",
    "### end of hack\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "import json\n",
    "import gensim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import objectpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = os.getcwd()\n",
    "dataDir = home + '/bioasq/data'\n",
    "modelsDir = home + '/bioasq/models/dan.h5'\n",
    "embeddingFile = home + '/bioasq/embeddings/pubmed2018_w2v_200D.bin'\n",
    "trainFilePath = dataDir + '/BioASQ-trainingDataset6b.json'\n",
    "trainJSON = {}\n",
    "no_vectors = {}\n",
    "yes_vectors = {}\n",
    "missing_vectors = {}\n",
    "embedding_dimension = 200\n",
    "labels = {\n",
    "    \"summary\": [1, 0, 0, 0],\n",
    "    \"list\":    [0, 1, 0, 0],\n",
    "    \"yesno\":   [0, 0, 1, 0],\n",
    "    \"factoid\": [0, 0, 0, 0]\n",
    "}\n",
    "label_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from files\n",
    "pubmedW2V = gensim.models.KeyedVectors.load_word2vec_format(embeddingFile, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(trainFilePath) as f:\n",
    "    trainJSON = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_punctuation(str):\n",
    "    pStr = str.lower()\n",
    "    pStr = \" \".join(re.findall(r\"[\\w']+|[.,!?;'s]\", pStr))\n",
    "    remove_chars = [\"'s\", \"'t\", \"s'\", \"'\", \",\", \".\",\"?\", \"!\"]\n",
    "    for ch in remove_chars:\n",
    "        pStr = pStr.replace(ch, \"\")\n",
    "    pStr = pStr.replace(\"  \", \" \")\n",
    "    return pStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ret gdnf ednrb edn3  and sox10 lead to long segment l hscr '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seperate_punctuation(\"RET, GDNF, EDNRB, EDN3!, ands' SOX10 lead's to't long-segment (L-HSCR)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getQuestionembeddingsFromText(qText):\n",
    "    global no_vectors\n",
    "    global yes_vectors\n",
    "    global missing_vectors\n",
    "    qVect = seperate_punctuation(qText)\n",
    "    qWords = qVect.split(\" \")\n",
    "    qEmbeddings = np.zeros((len(qWords), embedding_dimension))\n",
    "    for i, qWord in enumerate(qWords):\n",
    "        qEmbedding = np.zeros((1, embedding_dimension))\n",
    "        try:\n",
    "            qEmbeddings[i:] = pubmedW2V.get_vector(qWord)  \n",
    "            if qWord not in yes_vectors:\n",
    "                yes_vectors[qWord] = 0;\n",
    "            yes_vectors[qWord] += 1\n",
    "        except:\n",
    "            if qWord not in no_vectors:\n",
    "                no_vectors[qWord] = 0;\n",
    "            if qWord not in missing_vectors:\n",
    "                missing_vectors[qWord] = np.random.randn(1, embedding_dimension)\n",
    "            no_vectors[qWord] += 1\n",
    "            qEmbeddings[i:] = missing_vectors[qWord]        \n",
    "    qEmbeddings = np.mean(qEmbeddings, axis=0, keepdims=True)\n",
    "    return qEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   turbeculosis   path2ppi   kub5   mesaure   metazaon   sfpr3   archaelogy   secreatase   mirduplexsvm   phoshpatases   bouble   axagliptin   hydroxyisobutyrylation   thyroitoxicosis   itagliptin   hisrchsprung   alterred   dediodinases   μac1   mer41   chédiak   acetylgalactosaminidase   immonostaining   regioner   levoxyl   phopspholamban   mepopausal   6srna   glycolylneuraminic   arcalyst   sqtlseeker   ranasmurfin   menzerath   trigered   woolsorter   pregrancy   subtahalamic\n",
      "\n",
      "\n",
      "No vector count % = 0.8733624454148471\n"
     ]
    }
   ],
   "source": [
    "question_count = len(trainJSON[\"questions\"])\n",
    "question_embeddings = np.zeros((question_count, 1, embedding_dimension))\n",
    "question_labels = np.zeros((question_count, embedding_dimension))\n",
    "questions_data = []\n",
    "for i, question in enumerate(trainJSON[\"questions\"]):\n",
    "    embedding = getQuestionembeddingsFromText(question[\"body\"])\n",
    "    question_embeddings[i:] = embedding\n",
    "    question_labels[i:] = embedding\n",
    "    questions_data.append({\n",
    "        \"id\": question[\"id\"],\n",
    "        \"embedding\": embedding,\n",
    "        \"vector\": None\n",
    "    })\n",
    "    \n",
    "print(\"   \".join(no_vectors.keys()))\n",
    "n_cnt = len(no_vectors.keys())\n",
    "y_cnt = len(yes_vectors.keys())\n",
    "percent = ((n_cnt/(n_cnt+y_cnt)) * 100)\n",
    "print(\"\\n\\nNo vector count % = {}\".format(percent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2251, 1, 200)\n",
      "(2251, 200)\n",
      "[[-0.04032383  0.23722161  0.01415557 ... -0.20252093  0.2247508\n",
      "  -0.03295899]\n",
      " [-0.12237463  0.11521546 -0.09487724 ... -0.23687904 -0.03425544\n",
      "   0.04332822]\n",
      " [ 0.1490044  -0.03180349 -0.04777178 ... -0.32831423  0.13612961\n",
      "   0.21818388]\n",
      " ...\n",
      " [ 0.1391815   0.10629099  0.04947067 ... -0.25339634  0.27767004\n",
      "   0.02496959]\n",
      " [ 0.01561616  0.20751076  0.00118731 ... -0.37215197  0.52265794\n",
      "   0.16852258]\n",
      " [-0.03440449  0.06675705 -0.06633284 ... -0.30361671  0.17864662\n",
      "  -0.01184909]]\n"
     ]
    }
   ],
   "source": [
    "print(question_embeddings.shape)\n",
    "print(question_labels.shape)\n",
    "print(question_labels[:-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(input_shape):\n",
    "    X_input = keras.layers.Input(shape=input_shape)\n",
    "    X = keras.layers.Flatten()(X_input)\n",
    "    X = keras.layers.Dense(128, activation=tf.nn.relu)(X)\n",
    "    X = keras.layers.Dense(256, activation=tf.nn.relu)(X)\n",
    "    X = keras.layers.Dense(128, activation=tf.nn.relu)(X)\n",
    "    X = keras.layers.Dense(64, activation=tf.nn.relu)(X)\n",
    "    X = keras.layers.Dense(128, activation=tf.nn.relu)(X)\n",
    "    X = keras.layers.Dense(256, activation=tf.nn.relu)(X)\n",
    "    X = keras.layers.Dense(embedding_dimension, activation=tf.nn.relu)(X)\n",
    "    # X = keras.layers.Dense(label_dim, activation=tf.nn.sigmoid)(X)\n",
    "    model = keras.Model(inputs=X_input, outputs=X, name='HappyModel')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getModel((question_embeddings.shape[1], question_embeddings.shape[2]))\n",
    "model.compile(\n",
    "    optimizer=tf.train.AdamOptimizer(),\n",
    "    loss='mse',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 1, 200)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 128)               25728     \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 200)               51400     \n",
      "=================================================================\n",
      "Total params: 192,648\n",
      "Trainable params: 192,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(question_embeddings, question_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 1, 200)\n",
      "(1800, 200)\n",
      "(451, 1, 200)\n",
      "(451, 200)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1800/1800 [==============================] - 0s 112us/step - loss: 0.0159 - acc: 0.7683\n",
      "Epoch 2/150\n",
      "1800/1800 [==============================] - 0s 121us/step - loss: 0.0159 - acc: 0.7706\n",
      "Epoch 3/150\n",
      "1800/1800 [==============================] - 0s 121us/step - loss: 0.0159 - acc: 0.7567\n",
      "Epoch 4/150\n",
      "1800/1800 [==============================] - 0s 122us/step - loss: 0.0159 - acc: 0.7617\n",
      "Epoch 5/150\n",
      "1800/1800 [==============================] - 0s 126us/step - loss: 0.0159 - acc: 0.7728\n",
      "Epoch 6/150\n",
      "1800/1800 [==============================] - 0s 126us/step - loss: 0.0159 - acc: 0.7617\n",
      "Epoch 7/150\n",
      "1800/1800 [==============================] - 0s 118us/step - loss: 0.0159 - acc: 0.7683\n",
      "Epoch 8/150\n",
      "1800/1800 [==============================] - 0s 125us/step - loss: 0.0159 - acc: 0.7683\n",
      "Epoch 9/150\n",
      "1800/1800 [==============================] - 0s 130us/step - loss: 0.0159 - acc: 0.7689\n",
      "Epoch 10/150\n",
      "1800/1800 [==============================] - 0s 128us/step - loss: 0.0159 - acc: 0.7761\n",
      "Epoch 11/150\n",
      "1800/1800 [==============================] - 0s 128us/step - loss: 0.0159 - acc: 0.7678\n",
      "Epoch 12/150\n",
      "1800/1800 [==============================] - 0s 127us/step - loss: 0.0159 - acc: 0.7639\n",
      "Epoch 13/150\n",
      "1800/1800 [==============================] - 0s 122us/step - loss: 0.0159 - acc: 0.7600\n",
      "Epoch 14/150\n",
      "1800/1800 [==============================] - 0s 128us/step - loss: 0.0159 - acc: 0.7667\n",
      "Epoch 15/150\n",
      "1800/1800 [==============================] - 0s 133us/step - loss: 0.0159 - acc: 0.7572\n",
      "Epoch 16/150\n",
      "1800/1800 [==============================] - 0s 110us/step - loss: 0.0159 - acc: 0.7650\n",
      "Epoch 17/150\n",
      "1800/1800 [==============================] - 0s 127us/step - loss: 0.0159 - acc: 0.7700\n",
      "Epoch 18/150\n",
      "1800/1800 [==============================] - 0s 122us/step - loss: 0.0158 - acc: 0.7683\n",
      "Epoch 19/150\n",
      "1800/1800 [==============================] - 0s 115us/step - loss: 0.0159 - acc: 0.7844\n",
      "Epoch 20/150\n",
      "1800/1800 [==============================] - 0s 120us/step - loss: 0.0159 - acc: 0.7606\n",
      "Epoch 21/150\n",
      "1800/1800 [==============================] - 0s 117us/step - loss: 0.0159 - acc: 0.7706\n",
      "Epoch 22/150\n",
      "1800/1800 [==============================] - 0s 119us/step - loss: 0.0159 - acc: 0.7767\n",
      "Epoch 23/150\n",
      "1800/1800 [==============================] - 0s 127us/step - loss: 0.0159 - acc: 0.7706\n",
      "Epoch 24/150\n",
      "1800/1800 [==============================] - 0s 120us/step - loss: 0.0158 - acc: 0.7639\n",
      "Epoch 25/150\n",
      "1800/1800 [==============================] - 0s 115us/step - loss: 0.0159 - acc: 0.7539\n",
      "Epoch 26/150\n",
      "1800/1800 [==============================] - 0s 124us/step - loss: 0.0159 - acc: 0.7644\n",
      "Epoch 27/150\n",
      "1800/1800 [==============================] - 0s 121us/step - loss: 0.0159 - acc: 0.7633\n",
      "Epoch 28/150\n",
      "1800/1800 [==============================] - 0s 124us/step - loss: 0.0159 - acc: 0.7711\n",
      "Epoch 29/150\n",
      "1800/1800 [==============================] - 0s 121us/step - loss: 0.0159 - acc: 0.7606\n",
      "Epoch 30/150\n",
      "1800/1800 [==============================] - 0s 119us/step - loss: 0.0158 - acc: 0.7622\n",
      "Epoch 31/150\n",
      "1800/1800 [==============================] - 0s 124us/step - loss: 0.0158 - acc: 0.7733\n",
      "Epoch 32/150\n",
      "1800/1800 [==============================] - 0s 117us/step - loss: 0.0158 - acc: 0.7678\n",
      "Epoch 33/150\n",
      "1800/1800 [==============================] - 0s 119us/step - loss: 0.0158 - acc: 0.7633\n",
      "Epoch 34/150\n",
      "1800/1800 [==============================] - 0s 124us/step - loss: 0.0158 - acc: 0.7678\n",
      "Epoch 35/150\n",
      "1800/1800 [==============================] - 0s 116us/step - loss: 0.0159 - acc: 0.7633\n",
      "Epoch 36/150\n",
      "1800/1800 [==============================] - 0s 122us/step - loss: 0.0158 - acc: 0.7694\n",
      "Epoch 37/150\n",
      "1800/1800 [==============================] - 0s 118us/step - loss: 0.0158 - acc: 0.7700\n",
      "Epoch 38/150\n",
      "1800/1800 [==============================] - 0s 132us/step - loss: 0.0159 - acc: 0.7706\n",
      "Epoch 39/150\n",
      "1800/1800 [==============================] - 0s 114us/step - loss: 0.0159 - acc: 0.7650\n",
      "Epoch 40/150\n",
      "1800/1800 [==============================] - 0s 117us/step - loss: 0.0159 - acc: 0.7561\n",
      "Epoch 41/150\n",
      "1800/1800 [==============================] - 0s 113us/step - loss: 0.0159 - acc: 0.7661\n",
      "Epoch 42/150\n",
      "1800/1800 [==============================] - 0s 128us/step - loss: 0.0158 - acc: 0.7628\n",
      "Epoch 43/150\n",
      "1800/1800 [==============================] - 0s 116us/step - loss: 0.0158 - acc: 0.7739\n",
      "Epoch 44/150\n",
      "1800/1800 [==============================] - 0s 114us/step - loss: 0.0158 - acc: 0.7794\n",
      "Epoch 45/150\n",
      "1800/1800 [==============================] - 0s 122us/step - loss: 0.0159 - acc: 0.7661\n",
      "Epoch 46/150\n",
      "1800/1800 [==============================] - 0s 118us/step - loss: 0.0159 - acc: 0.7628\n",
      "Epoch 47/150\n",
      "1800/1800 [==============================] - 0s 115us/step - loss: 0.0159 - acc: 0.7656\n",
      "Epoch 48/150\n",
      "1800/1800 [==============================] - 0s 123us/step - loss: 0.0158 - acc: 0.7783\n",
      "Epoch 49/150\n",
      "1800/1800 [==============================] - 0s 114us/step - loss: 0.0158 - acc: 0.7672\n",
      "Epoch 50/150\n",
      "1800/1800 [==============================] - 0s 114us/step - loss: 0.0158 - acc: 0.7783\n",
      "Epoch 51/150\n",
      "1800/1800 [==============================] - 0s 127us/step - loss: 0.0158 - acc: 0.7783\n",
      "Epoch 52/150\n",
      "1800/1800 [==============================] - 0s 112us/step - loss: 0.0158 - acc: 0.7761\n",
      "Epoch 53/150\n",
      "1800/1800 [==============================] - 0s 114us/step - loss: 0.0159 - acc: 0.7689\n",
      "Epoch 54/150\n",
      "1800/1800 [==============================] - 0s 123us/step - loss: 0.0158 - acc: 0.7639\n",
      "Epoch 55/150\n",
      "1800/1800 [==============================] - 0s 120us/step - loss: 0.0158 - acc: 0.7756\n",
      "Epoch 56/150\n",
      "1800/1800 [==============================] - 0s 127us/step - loss: 0.0158 - acc: 0.7650\n",
      "Epoch 57/150\n",
      "1800/1800 [==============================] - 0s 123us/step - loss: 0.0158 - acc: 0.7656\n",
      "Epoch 58/150\n",
      "1800/1800 [==============================] - 0s 126us/step - loss: 0.0159 - acc: 0.7744\n",
      "Epoch 59/150\n",
      "1800/1800 [==============================] - 0s 121us/step - loss: 0.0158 - acc: 0.7728\n",
      "Epoch 60/150\n",
      "1800/1800 [==============================] - 0s 121us/step - loss: 0.0159 - acc: 0.7739\n",
      "Epoch 61/150\n",
      "1800/1800 [==============================] - 0s 113us/step - loss: 0.0158 - acc: 0.7711\n",
      "Epoch 62/150\n",
      "1800/1800 [==============================] - 0s 117us/step - loss: 0.0158 - acc: 0.7783\n",
      "Epoch 63/150\n",
      "1800/1800 [==============================] - 0s 111us/step - loss: 0.0158 - acc: 0.7800\n",
      "Epoch 64/150\n",
      "1800/1800 [==============================] - 0s 119us/step - loss: 0.0158 - acc: 0.7717\n",
      "Epoch 65/150\n",
      "1800/1800 [==============================] - 0s 113us/step - loss: 0.0158 - acc: 0.7706\n",
      "Epoch 66/150\n",
      "1800/1800 [==============================] - 0s 111us/step - loss: 0.0158 - acc: 0.7728\n",
      "Epoch 67/150\n",
      "1800/1800 [==============================] - 0s 121us/step - loss: 0.0159 - acc: 0.7656\n",
      "Epoch 68/150\n",
      "1800/1800 [==============================] - 0s 108us/step - loss: 0.0158 - acc: 0.7628\n",
      "Epoch 69/150\n",
      "1800/1800 [==============================] - 0s 111us/step - loss: 0.0158 - acc: 0.7850\n",
      "Epoch 70/150\n",
      "1800/1800 [==============================] - 0s 117us/step - loss: 0.0158 - acc: 0.7811\n",
      "Epoch 71/150\n",
      "1800/1800 [==============================] - 0s 114us/step - loss: 0.0158 - acc: 0.7767\n",
      "Epoch 72/150\n",
      "1800/1800 [==============================] - 0s 118us/step - loss: 0.0158 - acc: 0.7761\n",
      "Epoch 73/150\n",
      "1800/1800 [==============================] - 0s 110us/step - loss: 0.0158 - acc: 0.7661\n",
      "Epoch 74/150\n",
      "1800/1800 [==============================] - 0s 104us/step - loss: 0.0158 - acc: 0.7817\n",
      "Epoch 75/150\n",
      "1800/1800 [==============================] - 0s 117us/step - loss: 0.0158 - acc: 0.7606\n",
      "Epoch 76/150\n",
      "1800/1800 [==============================] - 0s 116us/step - loss: 0.0158 - acc: 0.7844\n",
      "Epoch 77/150\n",
      "1800/1800 [==============================] - 0s 114us/step - loss: 0.0158 - acc: 0.7806\n",
      "Epoch 78/150\n",
      "1800/1800 [==============================] - 0s 118us/step - loss: 0.0158 - acc: 0.7761\n",
      "Epoch 79/150\n",
      "1800/1800 [==============================] - 0s 113us/step - loss: 0.0158 - acc: 0.7650\n",
      "Epoch 80/150\n",
      "1800/1800 [==============================] - 0s 111us/step - loss: 0.0158 - acc: 0.7700\n",
      "Epoch 81/150\n",
      "1800/1800 [==============================] - 0s 117us/step - loss: 0.0158 - acc: 0.7733\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 0s 109us/step - loss: 0.0158 - acc: 0.7667\n",
      "Epoch 83/150\n",
      "1800/1800 [==============================] - 0s 111us/step - loss: 0.0158 - acc: 0.7717\n",
      "Epoch 84/150\n",
      "1800/1800 [==============================] - 0s 127us/step - loss: 0.0158 - acc: 0.7839\n",
      "Epoch 85/150\n",
      "1800/1800 [==============================] - 0s 120us/step - loss: 0.0159 - acc: 0.7661\n",
      "Epoch 86/150\n",
      "1800/1800 [==============================] - 0s 124us/step - loss: 0.0158 - acc: 0.7722\n",
      "Epoch 87/150\n",
      "1800/1800 [==============================] - 0s 119us/step - loss: 0.0158 - acc: 0.7706\n",
      "Epoch 88/150\n",
      "1800/1800 [==============================] - 0s 124us/step - loss: 0.0158 - acc: 0.7739\n",
      "Epoch 89/150\n",
      "1800/1800 [==============================] - 0s 121us/step - loss: 0.0158 - acc: 0.7767\n",
      "Epoch 90/150\n",
      "1800/1800 [==============================] - 0s 118us/step - loss: 0.0158 - acc: 0.7711\n",
      "Epoch 91/150\n",
      "1800/1800 [==============================] - 0s 124us/step - loss: 0.0158 - acc: 0.7689\n",
      "Epoch 92/150\n",
      "1800/1800 [==============================] - 0s 118us/step - loss: 0.0158 - acc: 0.7778\n",
      "Epoch 93/150\n",
      "1800/1800 [==============================] - 0s 121us/step - loss: 0.0158 - acc: 0.7789\n",
      "Epoch 94/150\n",
      "1800/1800 [==============================] - 0s 122us/step - loss: 0.0158 - acc: 0.7761\n",
      "Epoch 95/150\n",
      "1800/1800 [==============================] - 0s 120us/step - loss: 0.0158 - acc: 0.7872\n",
      "Epoch 96/150\n",
      "1800/1800 [==============================] - 0s 118us/step - loss: 0.0158 - acc: 0.7756\n",
      "Epoch 97/150\n",
      "1800/1800 [==============================] - 0s 122us/step - loss: 0.0158 - acc: 0.7800\n",
      "Epoch 98/150\n",
      "1800/1800 [==============================] - 0s 113us/step - loss: 0.0158 - acc: 0.7783\n",
      "Epoch 99/150\n",
      "1800/1800 [==============================] - 0s 116us/step - loss: 0.0158 - acc: 0.7750\n",
      "Epoch 100/150\n",
      "1800/1800 [==============================] - 0s 116us/step - loss: 0.0158 - acc: 0.7778\n",
      "Epoch 101/150\n",
      "1800/1800 [==============================] - 0s 116us/step - loss: 0.0158 - acc: 0.7750\n",
      "Epoch 102/150\n",
      "1800/1800 [==============================] - 0s 124us/step - loss: 0.0158 - acc: 0.7767\n",
      "Epoch 103/150\n",
      "1800/1800 [==============================] - 0s 118us/step - loss: 0.0158 - acc: 0.7756\n",
      "Epoch 104/150\n",
      "1800/1800 [==============================] - 0s 115us/step - loss: 0.0158 - acc: 0.7772\n",
      "Epoch 105/150\n",
      "1800/1800 [==============================] - 0s 125us/step - loss: 0.0158 - acc: 0.7772\n",
      "Epoch 106/150\n",
      "1800/1800 [==============================] - 0s 122us/step - loss: 0.0158 - acc: 0.7778\n",
      "Epoch 107/150\n",
      "1800/1800 [==============================] - 0s 116us/step - loss: 0.0158 - acc: 0.7811\n",
      "Epoch 108/150\n",
      "1800/1800 [==============================] - 0s 126us/step - loss: 0.0158 - acc: 0.7833\n",
      "Epoch 109/150\n",
      "1800/1800 [==============================] - 0s 119us/step - loss: 0.0158 - acc: 0.7772\n",
      "Epoch 110/150\n",
      "1800/1800 [==============================] - 0s 122us/step - loss: 0.0158 - acc: 0.7800\n",
      "Epoch 111/150\n",
      "1800/1800 [==============================] - 0s 123us/step - loss: 0.0158 - acc: 0.7683\n",
      "Epoch 112/150\n",
      "1800/1800 [==============================] - 0s 113us/step - loss: 0.0158 - acc: 0.7856\n",
      "Epoch 113/150\n",
      "1800/1800 [==============================] - 0s 114us/step - loss: 0.0158 - acc: 0.7717\n",
      "Epoch 114/150\n",
      "1800/1800 [==============================] - 0s 123us/step - loss: 0.0158 - acc: 0.7750\n",
      "Epoch 115/150\n",
      "1800/1800 [==============================] - 0s 116us/step - loss: 0.0158 - acc: 0.7772\n",
      "Epoch 116/150\n",
      "1800/1800 [==============================] - 0s 122us/step - loss: 0.0158 - acc: 0.7800\n",
      "Epoch 117/150\n",
      "1800/1800 [==============================] - 0s 114us/step - loss: 0.0158 - acc: 0.7689\n",
      "Epoch 118/150\n",
      "1800/1800 [==============================] - 0s 116us/step - loss: 0.0158 - acc: 0.7850\n",
      "Epoch 119/150\n",
      "1800/1800 [==============================] - 0s 116us/step - loss: 0.0158 - acc: 0.7733\n",
      "Epoch 120/150\n",
      "1800/1800 [==============================] - 0s 116us/step - loss: 0.0158 - acc: 0.7839\n",
      "Epoch 121/150\n",
      "1800/1800 [==============================] - 0s 118us/step - loss: 0.0158 - acc: 0.7806\n",
      "Epoch 122/150\n",
      "1800/1800 [==============================] - 0s 120us/step - loss: 0.0158 - acc: 0.7794\n",
      "Epoch 123/150\n",
      "1800/1800 [==============================] - 0s 118us/step - loss: 0.0158 - acc: 0.7778\n",
      "Epoch 124/150\n",
      "1800/1800 [==============================] - 0s 116us/step - loss: 0.0158 - acc: 0.7733\n",
      "Epoch 125/150\n",
      "1800/1800 [==============================] - 0s 114us/step - loss: 0.0158 - acc: 0.7861\n",
      "Epoch 126/150\n",
      "1800/1800 [==============================] - 0s 122us/step - loss: 0.0158 - acc: 0.7844\n",
      "Epoch 127/150\n",
      "1800/1800 [==============================] - 0s 110us/step - loss: 0.0158 - acc: 0.7889\n",
      "Epoch 128/150\n",
      "1800/1800 [==============================] - 0s 116us/step - loss: 0.0158 - acc: 0.7778\n",
      "Epoch 129/150\n",
      "1800/1800 [==============================] - 0s 115us/step - loss: 0.0158 - acc: 0.7772\n",
      "Epoch 130/150\n",
      "1800/1800 [==============================] - 0s 114us/step - loss: 0.0158 - acc: 0.7722\n",
      "Epoch 131/150\n",
      "1800/1800 [==============================] - 0s 119us/step - loss: 0.0158 - acc: 0.7772\n",
      "Epoch 132/150\n",
      "1800/1800 [==============================] - 0s 121us/step - loss: 0.0158 - acc: 0.7850\n",
      "Epoch 133/150\n",
      "1800/1800 [==============================] - 0s 116us/step - loss: 0.0158 - acc: 0.7733\n",
      "Epoch 134/150\n",
      "1800/1800 [==============================] - 0s 106us/step - loss: 0.0158 - acc: 0.7750\n",
      "Epoch 135/150\n",
      "1800/1800 [==============================] - 0s 119us/step - loss: 0.0158 - acc: 0.7817\n",
      "Epoch 136/150\n",
      "1800/1800 [==============================] - 0s 112us/step - loss: 0.0158 - acc: 0.7811\n",
      "Epoch 137/150\n",
      "1800/1800 [==============================] - 0s 117us/step - loss: 0.0158 - acc: 0.7722\n",
      "Epoch 138/150\n",
      "1800/1800 [==============================] - 0s 115us/step - loss: 0.0158 - acc: 0.7844\n",
      "Epoch 139/150\n",
      "1800/1800 [==============================] - 0s 118us/step - loss: 0.0158 - acc: 0.7828\n",
      "Epoch 140/150\n",
      "1800/1800 [==============================] - 0s 113us/step - loss: 0.0158 - acc: 0.7811\n",
      "Epoch 141/150\n",
      "1800/1800 [==============================] - 0s 114us/step - loss: 0.0158 - acc: 0.7844\n",
      "Epoch 142/150\n",
      "1800/1800 [==============================] - 0s 123us/step - loss: 0.0158 - acc: 0.7806\n",
      "Epoch 143/150\n",
      "1800/1800 [==============================] - 0s 121us/step - loss: 0.0158 - acc: 0.7772\n",
      "Epoch 144/150\n",
      "1800/1800 [==============================] - 0s 117us/step - loss: 0.0158 - acc: 0.7744\n",
      "Epoch 145/150\n",
      "1800/1800 [==============================] - 0s 109us/step - loss: 0.0158 - acc: 0.7822\n",
      "Epoch 146/150\n",
      "1800/1800 [==============================] - 0s 112us/step - loss: 0.0158 - acc: 0.7806\n",
      "Epoch 147/150\n",
      "1800/1800 [==============================] - 0s 115us/step - loss: 0.0158 - acc: 0.7861\n",
      "Epoch 148/150\n",
      "1800/1800 [==============================] - 0s 125us/step - loss: 0.0158 - acc: 0.7733\n",
      "Epoch 149/150\n",
      "1800/1800 [==============================] - 0s 106us/step - loss: 0.0158 - acc: 0.7728\n",
      "Epoch 150/150\n",
      "1800/1800 [==============================] - 0s 112us/step - loss: 0.0158 - acc: 0.7744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f27973b7f98>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train, y=y_train, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451/451 [==============================] - 0s 374us/step\n",
      "Loss = 0.02  and test accuracy = 0.63\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Loss = %.2f  and test accuracy = %.2f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPenultimateLayerOutput(model, input_embedding):\n",
    "    index = len(model.layers) - 3\n",
    "    get_2ndlast_layer_output = K.function([model.layers[0].input],\n",
    "                                      [model.layers[index].output])\n",
    "    layer_output = get_2ndlast_layer_output([input_embedding])[0]\n",
    "    return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_vectors = getPenultimateLayerOutput(model, question_embeddings)\n",
    "for i, qVect in enumerate(question_vectors):\n",
    "    questions_data[i][\"vector\"] = qVect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2251, 128)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPenultimateLayerOutput(model, question_embeddings).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQueryVector(qText):\n",
    "    qEmbedding = getQuestionembeddingsFromText(qText)\n",
    "    qVector = getPenultimateLayerOutput(model, qEmbedding)\n",
    "    return qVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTop10MatchingQuestionIdsForQuery(qText):\n",
    "    qVector = getQueryVector(qText)\n",
    "    values = []\n",
    "    dtype = [('id', 'S50'), ('distance', float)]\n",
    "    for i, question in enumerate(questions_data):\n",
    "        values.append((\n",
    "            question[\"id\"],\n",
    "            cosine_similarity(qVector, question[\"vector\"].reshape((1, 128))).squeeze()\n",
    "        ))\n",
    "    distances = np.array(values, dtype=dtype)\n",
    "    questionid_cosine_distances = np.sort(distances, order=[\"distance\"])\n",
    "    return questionid_cosine_distances[-10:][::-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeQuery(qText):\n",
    "    top_matches = getTop10MatchingQuestionIdsForQuery(qText)\n",
    "\n",
    "    for i, (id, dist) in enumerate(top_matches):\n",
    "        for question in trainJSON[\"questions\"]:\n",
    "            if id.decode(\"utf-8\") == question[\"id\"]:\n",
    "                print(\"\\nActual Question => \" + question[\"body\"])\n",
    "                print(\"\\n\\n\")\n",
    "                print(question[\"ideal_answer\"])\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actual Question => Which drugs may interfere thyroxine absorption?\n",
      "\n",
      "\n",
      "\n",
      "['bile acid sequestrants, ferrous sulphate, sucralfate, calcium carbonate, aluminium-containing antacids, phosphate binders, raloxifene and proton-pump inhibitors, have also been shown to interfere with the absorption of levothyroxine\\nsevelamer hydrochloride or chromium picolinate should be advised to separate the time of ingestion of these drugs from their thyroid hormone preparation by several hours']\n",
      "\n",
      "Actual Question => Does metformin interfere thyroxine absorption?\n",
      "\n",
      "\n",
      "\n",
      "['There are not reported data indicating that metformin interferes with thyroxine absorption']\n",
      "\n",
      "Actual Question => What are reactive metabolites?\n",
      "\n",
      "\n",
      "\n",
      "['Reactive metabolites are generated when a small molecule, commonly a drug or hydrocarbon, is broken down in the body. Reactive metabolites can cause cancer and other diseases as well as hepatoxicty. ']\n",
      "\n",
      "Actual Question => Have thyronamines effects on fat tissue?\n",
      "\n",
      "\n",
      "\n",
      "['thyronamines cause reduction of fat mass']\n",
      "\n",
      "Actual Question => Have thyronamines  effects on fat tissue?\n",
      "\n",
      "\n",
      "\n",
      "['There is not clear evidence that thyronamines have direct effect on adipose tissue']\n",
      "\n",
      "Actual Question => What is oprozomib?\n",
      "\n",
      "\n",
      "\n",
      "['Oprozomib is a second-generation, highly-selective, orally administered proteasome inhibitor with promising activity against multiple myeloma.\\nOprozomib directly inhibited OC formation and bone resorption in vitro, while enhancing osteogenic differentiation and matrix mineralization. Oprozomib increased trabecular bone volume, decreased bone resorption and enhanced bone formation in non-tumor bearing mice. Consequently, oprozomib seems to be able to effectively shift the bone microenvironment from a catabolic to an anabolic state and, similar to bortezomib, may decrease skeletal complications of MM.\\nOprozomib effectively decreases multiple myeloma cell viability.\\nOprozomib potently inhibit cell survival and induce apoptosis in HNSCC cell lines via upregulation of pro-apoptotic Bik. Upregulation of Mcl-1 by these agents served to dampen their efficacies. Oprozomib also induced autophagy, mediated, in part, by activation of the UPR pathway involving upregulation of ATF4 transcription factor. Autophagy induction served a prosurvival role. Oral administration of ONX 0912 inhibited the growth of HNSCC xenograft tumors in a dose-dependent manner.\\nOprozomib inhibited NF-κB expression.']\n",
      "\n",
      "Actual Question => What enzyme is inhibied by Opicapone?\n",
      "\n",
      "\n",
      "\n",
      "[\"Opicapone is a novel catechol-O-methyltransferase (COMT) inhibitor to be used as adjunctive therapy in levodopa-treated patients with Parkinson's disease\"]\n",
      "\n",
      "Actual Question => Is Doxorubicin cardiotoxic?\n",
      "\n",
      "\n",
      "\n",
      "['Doxorubicin (DOXO) is widely used to treat solid tumors. However, its clinical use is limited by side effects including serious cardiotoxicity due to cardiomyocyte damage', 'Doxorubicin (DOXO) is widely used to treat solid tumors. However, its clinical use is limited by side effects including serious cardiotoxicity due to cardiomyocyte damage. ']\n",
      "\n",
      "Actual Question => Which are the cardiac effects of thyronamines?\n",
      "\n",
      "\n",
      "\n",
      "['Thyronamines have negative chronotropy, negative inotropy; in particular thyronamines are considered negative inotropic agents', 'In the heart, thyronamines cause negative chronotropy, negative inotropy,reduced cardiac output and resistance to ischemic injury.']\n",
      "\n",
      "Actual Question => What is Desomorphine?\n",
      "\n",
      "\n",
      "\n",
      "['Desomorphine is an opioid misused as \"crocodile\", a cheaper alternative to heroin Desomorphine is the semi-synthetic opioid claimed to be the main component of krokodil', 'Desomorphine is an opioid misused as \"crocodile\", a cheaper alternative to heroin', 'Desomorphine is the semi-synthetic opioid claimed to be the main component of krokodil', '\"Krokodil\" is the street name for the homemade injectable mixture that has been used as a cheap substitute for heroin. Desomorphine is an opioid misused as \"crocodile\", a cheaper alternative to heroin.', 'Desomorphine is the semi-synthetic opioid claimed to be the main component of krokodil Desomorphine is an opioid misused as \"crocodile\", a cheaper alternative to heroin', 'Desomorphine is an opioid drug which is often synthsised using a combination of readily available ingredients and is often available on the \"street\" as a cheaper alternative to heroin.']\n"
     ]
    }
   ],
   "source": [
    "# Actual question from data => Which drugs may interfere thyroxine absorption?\n",
    "# Text input to query       => thyroxine absorption?  (a partial representation of exact query)\n",
    "\n",
    "executeQuery(\"thyroxine absorption?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actual Question => Is the protein Papilin secreted?\n",
      "\n",
      "\n",
      "\n",
      "['Yes,  papilin is a secreted protein']\n",
      "\n",
      "Actual Question => What is TOPAZ1?\n",
      "\n",
      "\n",
      "\n",
      "['TOPAZ1 is a novel germ cell-specific expressed gene conserved during evolution across vertebrates. Its PAZ-domain protein is abundantly expressed in the gonads during germ cell meiosis. The expression pattern of TOPAZ1, and its high degree of conservation, suggests that it may play an important role in germ cell development. Further characterization of TOPAZ1 may elucidate the mechanisms involved in gametogenesis, and particularly in the RNA silencing process in the germ line.', 'TOPAZ1 (Testis and Ovary-specific PAZ domain gene 1) is a germ cell specific factor that is essential for male meiotic progression. Topaz1 is supposed to have a role during gametogenesis and may be involved in the piRNA pathway and contribute to silencing of transposable elements and maintenance of genome integrity. It is highly conserved in vertebrates.']\n",
      "\n",
      "Actual Question => Are cyclophilins ubiquitously expressed?\n",
      "\n",
      "\n",
      "\n",
      "['Yes, \\tcyps (cyclophilins) are ubiquitous proteins of the immunophilin superfamily.']\n",
      "\n",
      "Actual Question => What is REVIGO?\n",
      "\n",
      "\n",
      "\n",
      "['REVIGO summarizes and visualizes long lists of gene ontology terms.', 'REVIGO is a Web server that summarizes long, unintelligible lists of GO terms by finding a representative subset of the terms using a simple clustering algorithm that relies on semantic similarity measures. Furthermore, REVIGO visualizes this non-redundant GO term set in multiple ways to assist in interpretation: multidimensional scaling and graph-based visualizations accurately render the subdivisions and the semantic relationships in the data, while treemaps and tag clouds are also offered as alternative views. REVIGO is freely available at http://revigo.irb.hr/.', 'Outcomes of high-throughput biological experiments are typically interpreted by statistical testing for enriched gene functional categories defined by the Gene Ontology (GO). The resulting lists of GO terms may be large and highly redundant, and thus difficult to interpret.REVIGO is a Web server that summarizes long, unintelligible lists of GO terms by finding a representative subset of the terms using a simple clustering algorithm that relies on semantic similarity measures. Furthermore, REVIGO visualizes this non-redundant GO term set in multiple ways to assist in interpretation: multidimensional scaling and graph-based visualizations accurately render the subdivisions and the semantic relationships in the data, while treemaps and tag clouds are also offered as alternative views. REVIGO is freely available at http://revigo.irb.hr/.']\n",
      "\n",
      "Actual Question => What is DECKO?\n",
      "\n",
      "\n",
      "\n",
      "['DECKO (Double Excision CRISPR Knockout) is a dual CRISPR tool, which is cloned using a single starting oligonucleotide, thereby affording simplicity and scalability to CRISPR knockout studies of non-coding genomic elements, including long non-coding RNAs.']\n",
      "\n",
      "Actual Question => What is TFBSshape?\n",
      "\n",
      "\n",
      "\n",
      "['To utilize DNA shape information when analysing the DNA binding specificities of TFs, the TFBSshape database was developed for calculating DNA structural features from nucleotide sequences provided by motif databases. The TFBSshape database can be used to generate heat maps and quantitative data for DNA structural features (i.e., minor groove width, roll, propeller twist and helix twist) for 739 TF datasets from 23 different species derived from the motif databases JASPAR and UniPROBE. As demonstrated for the basic helix-loop-helix and homeodomain TF families, TFBSshape database can be used to compare, qualitatively and quantitatively, the DNA binding specificities of closely related TFs and, thus, uncover differential DNA binding specificities that are not apparent from nucleotide sequence alone.']\n",
      "\n",
      "Actual Question => What is GDF10?\n",
      "\n",
      "\n",
      "\n",
      "['The growth/differentiation factor-10 (GDF-10) is a new member of the transforming growth factor-beta (TGF-beta) superfamily. It is highly related to bone morphogenetic protein-3 (BMP-3) and often referred to as BMP3b. The nucleotide sequence of GDF-10 encodes a predicted protein of 476 amino acids with a molecular weight of approximately 52,000. The GDF-10 polypeptide contains a potential signal sequence for secretion, a putative RXXR proteolytic processing site, and a carboxy-terminal domain with considerable homology to other known members of the TGF-beta superfamily. GDF10 is found primarily in murine uterus, adipose tissue, and brain and to a lesser extent in liver and spleen. In addition, GDF-10 mRNA was present in both neonatal and adult bone samples, with higher levels being detected in calvaria than in long bone. These results suggest that GDF10 may play multiple roles in regulating cell differentiation events, including those involved in skeletal morphogenesis. Gdf10 was mapped to the proximal region of mouse chromosome 14 close to a region known to contain a spontaneous recessive mutation that is associated with a craniofacial defect.']\n",
      "\n",
      "Actual Question => What is ChIPpeakAnno?\n",
      "\n",
      "\n",
      "\n",
      "['ChIPpeakAnno is a Bioconductor package within the statistical programming environment R that facilitates batch annotation of enriched peaks identified from ChIP-seq, ChIP-chip, cap analysis of gene expression (CAGE) or any experiments resulting in a large number of enriched genomic regions.']\n",
      "\n",
      "Actual Question => What is HbVar?\n",
      "\n",
      "\n",
      "\n",
      "['HbVar (http://globin.cse.psu.edu) is a relational database of hemoglobin variants and thalassemia mutations. Extensive information is recorded for each variant and mutation, including a description of the variant and associated pathology, hematology, electrophoretic mobility, methods of isolation, stability information, ethnic occurrence, structure studies, functional studies, and references. The initial information was derived from books by Dr. Titus Huisman and colleagues [Huisman et al., 1996, 1997, 1998]. The current database is updated regularly with the addition of new data and corrections to previous data. Queries can be formulated based on fields in the database. Tables of common categories of variants, such as all those involving the alpha1-globin gene (HBA1) or all those that result in high oxygen affinity, are maintained by automated queries on the database. Users can formulate more precise queries, such as identifying \"all beta-globin variants associated with instability and found in Scottish populations.\" This new database should be useful for clinical diagnosis as well as in fundamental studies of hemoglobin biochemistry, globin gene regulation, and human sequence variation at these loci.']\n",
      "\n",
      "Actual Question => What is HOCOMOCO?\n",
      "\n",
      "\n",
      "\n",
      "['HOCOMOCO is a comprehensive collection of human transcription factor binding sites models constructed by integration of binding sequences obtained by both low- and high-throughput methods. HOCOMOCO contains 426 systematically curated TFBS models for 401 human TFs, where 172 models are based on more than one data source.']\n"
     ]
    }
   ],
   "source": [
    "# Actual question from data => Is the protein Papilin secreted?\n",
    "# Text input to query       => Papilin secreted?  (a partial representation of exact query)\n",
    "\n",
    "executeQuery(\"Papilin secreted?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actual Question => List signaling molecules (ligands) that interact with the receptor EGFR?\n",
      "\n",
      "\n",
      "\n",
      "['The 7 known EGFR ligands  are: epidermal growth factor (EGF), betacellulin (BTC), epiregulin (EPR), heparin-binding EGF (HB-EGF), transforming growth factor-α [TGF-α], amphiregulin (AREG) and epigen (EPG).']\n",
      "\n",
      "Actual Question => List receptors of the drug Cilengitide\n",
      "\n",
      "\n",
      "\n",
      "['Cilengitide binds αvβ3 and αvβ5 integrins. It inhibits attachment and invasion of malignant cells. Thus, cilengitide is being tested for treatment of cancer patients.']\n",
      "\n",
      "Actual Question => Which signaling pathway is activating the dishevelled proteins?\n",
      "\n",
      "\n",
      "\n",
      "['Dishevelled (Xdsh) controls cell fate via canonical Wnt signaling']\n",
      "\n",
      "Actual Question => Which proteins act as histone-like molecules in prokaryotes?\n",
      "\n",
      "\n",
      "\n",
      "['Prokaryotic histone-like proteins (Hlps) or nucleoid-associated proteins (NAPs) are abundant proteins found in bacterial and plastid nucleoids. HU protein is a small, basic, heat-stable DNA-binding protein that is well-conserved in prokaryotes and is associated with the bacterial nucleoid. HU is well conserved in all prokaryotes but surprisingly, it is also homologous to another E. coli DNA-binding protein, IHF. In prokaryotes, IHF and HU are key architectural proteins present at high concentrations. Histone-like Nucleoid Structuring (H-NS) protein can facilitate correct recognition of a promoter by RNA polymerase in AT-rich gene regulatory regions', 'THe histone-like proteins HU, IHF, H-NS (Nucleoid Structuring) act as histones in prokaryotes']\n",
      "\n",
      "Actual Question => List scaffold proteins of the ERK signaling pathway.\n",
      "\n",
      "\n",
      "\n",
      "['Originally identified in yeast, scaffold proteins are now recognized to contribute to the specificity of MEK/ERK pathways in mammalian cells. These scaffolds include kinase suppressor of Ras (KSR), beta-arrestin, MEK partner-1 (MP-1), Sef and IQ motif-containing GTPase-activating protein 1(IQGAP1). Human disc-large homolog (hDlg) acts as a MEK2-specific scaffold protein for the ERK signaling pathway. Two scaffold proteins, caveolin-1 and IQGAP1, are required for phosphorylation of the actin associated pool of extracellular signal regulated kinase 1 and 2 (ERK1/2). Several 14-3-3 isotypes bind to protein kinase C (PKC)-zeta and facilitate coupling of PKC-zeta to Raf-1 an event that boosts the mitogen-activated protein kinase (ERK) pathway.', 'kinase suppressor of Ras 1\\nMEK Partner 1\\nBeta-arrestin\\nIQ motif containing GTPase-activating protein 1\\nkinase suppressor of Ras 2\\nmitogen-activated protein kinase organizer 1']\n",
      "\n",
      "Actual Question => Could transcription factors act as cell-cell signalling molecules?\n",
      "\n",
      "\n",
      "\n",
      "['Yes. Recent data support the view that transcription factors - in particular, homeoproteins - can be transferred from cell to cell and have direct non-cell-autonomous (and therefore paracrine) activities.', 'Pax6 is a transcription factor essential for the development of tissues including the eyes, central nervous system and endocrine glands of vertebrates and invertebrates. It regulates the expression of a broad range of molecules, including transcription factors, cell adhesion and short-range cell-cell signalling molecules, hormones and structural proteins']\n",
      "\n",
      "Actual Question => List available genetic multicolor cell labeling techiniques in Drosophila\n",
      "\n",
      "\n",
      "\n",
      "['Flybow and Drosophila Brainbow.']\n",
      "\n",
      "Actual Question => What kind of bonds are connecting keratin molecules?\n",
      "\n",
      "\n",
      "\n",
      "['cystine disulfide bonds\\namide bonds\\nhydrogen bonds']\n",
      "\n",
      "Actual Question => Is  farnesoid X receptor (FXR) a nuclear receptor?\n",
      "\n",
      "\n",
      "\n",
      "['Yes, farnesoid X receptor (FXR) is a nuclear receptor.']\n",
      "\n",
      "Actual Question => Which are the mammalian orthologs of Drosophila Yki?\n",
      "\n",
      "\n",
      "\n",
      "['There are two mammalian orthologs of Yki: YAP and TAZ']\n"
     ]
    }
   ],
   "source": [
    "# Actual question from data => List signaling molecules (ligands) that interact with the receptor EGFR?\"?\n",
    "# Text input to query       => List signaling molecules (ligands) (a partial representation of exact query)\n",
    "\n",
    "executeQuery(\"List signaling molecules (ligands)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
